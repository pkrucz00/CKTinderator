{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    " - Folder with files\n",
    " - Indices of shuffled files\n",
    " - Output folder\n",
    " - Range of files to be used\n",
    " - Batch size\n",
    " - Save every n batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f38a026dfb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import errno\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from urllib.parse import urlparse\n",
    "from math import log2\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from tqdm import tqdm\n",
    "from kornia.filters import filter2d\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torch.hub import download_url_to_file, HASH_REGEX\n",
    "try:\n",
    "    from torch.hub import get_dir\n",
    "except BaseException:\n",
    "    from torch.hub import _get_torch_home as get_dir\n",
    "\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_FOLDER = \"/media/pawel/DATA/tmp/freddie_mercuries/en_face/aligned\"\n",
    "glob_name = f\"{INPUT_FOLDER}/*.jpg\"\n",
    "path_names = [Path(p) for p in sorted(glob(glob_name))]\n",
    "assert path_names, \"path_names cannot be empty, check your input folder name\"\n",
    "\n",
    "OUTPUT_FOLDER = \"/media/pawel/DATA/tmp/freddie_mercuries/en_face/vectors\"\n",
    "\n",
    "IMG_SIZE = 256\n",
    "LATENT_DIM = 256\n",
    "\n",
    "START_FROM = 11300\n",
    "N_BATCHES = 200\n",
    "BATCH_SIZE = 5    # CHANGE TO A DIVISOR OF 100 fi.e. 2, 4, 5, 10, \n",
    "SAVE_EVERY_N_BATCHES = 100 // BATCH_SIZE\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "permutation_array = np.load(\"permuted_100000.npy\")\n",
    "assert path_names, \"permutation array cannot be empty, check your input folder name\"\n",
    "\n",
    "path_names_permuted = [path_names[i] for i in permutation_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def get_1d_dct(i, freq, L):\n",
    "    result = math.cos(math.pi * freq * (i + 0.5) / L) / math.sqrt(L)\n",
    "    return result * (1 if freq == 0 else math.sqrt(2))\n",
    "\n",
    "def is_power_of_two(val):\n",
    "    return log2(val).is_integer()\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def get_dct_weights(width, channel, fidx_u, fidx_v):\n",
    "    dct_weights = torch.zeros(1, channel, width, width)\n",
    "    c_part = channel // len(fidx_u)\n",
    "\n",
    "    for i, (u_x, v_y) in enumerate(zip(fidx_u, fidx_v)):\n",
    "        for x in range(width):\n",
    "            for y in range(width):\n",
    "                coor_value = get_1d_dct(x, u_x, width) * get_1d_dct(y, v_y, width)\n",
    "                dct_weights[:, i * c_part: (i + 1) * c_part, x, y] = coor_value\n",
    "\n",
    "    return dct_weights\n",
    "\n",
    "def Conv2dSame(dim_in, dim_out, kernel_size, bias = True):\n",
    "    pad_left = kernel_size // 2\n",
    "    pad_right = (pad_left - 1) if (kernel_size % 2) == 0 else pad_left\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.ZeroPad2d((pad_left, pad_right, pad_left, pad_right)),\n",
    "        nn.Conv2d(dim_in, dim_out, kernel_size, bias = bias)\n",
    "    )\n",
    "\n",
    "class ChanNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = ChanNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "\n",
    "class DepthWiseConv2d(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding = 0, stride = 1, bias = True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias = bias),\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, dim_head = 64, heads = 8, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nonlin = nn.GELU()\n",
    "\n",
    "        self.to_lin_q = nn.Conv2d(dim, inner_dim, 1, bias = False)\n",
    "        self.to_lin_kv = DepthWiseConv2d(dim, inner_dim * 2, 3, padding = 1, bias = False)\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, inner_dim, 1, bias = False)\n",
    "        self.to_kv = nn.Conv2d(dim, inner_dim * 2, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Conv2d(inner_dim * 2, dim, 1)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        h, x, y = self.heads, *fmap.shape[-2:]\n",
    "\n",
    "        # linear attention\n",
    "\n",
    "        lin_q, lin_k, lin_v = (self.to_lin_q(fmap), *self.to_lin_kv(fmap).chunk(2, dim = 1))\n",
    "        lin_q, lin_k, lin_v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = h), (lin_q, lin_k, lin_v))\n",
    "\n",
    "        lin_q = lin_q.softmax(dim = -1)\n",
    "        lin_k = lin_k.softmax(dim = -2)\n",
    "\n",
    "        lin_q = lin_q * self.scale\n",
    "\n",
    "        context = einsum('b n d, b n e -> b d e', lin_k, lin_v)\n",
    "        lin_out = einsum('b n d, b d e -> b n e', lin_q, context)\n",
    "        lin_out = rearrange(lin_out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
    "\n",
    "        # conv-like full attention\n",
    "\n",
    "        q, k, v = (self.to_q(fmap), *self.to_kv(fmap).chunk(2, dim = 1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) c x y', h = h), (q, k, v))\n",
    "\n",
    "        k = F.unfold(k, kernel_size = self.kernel_size, padding = self.kernel_size // 2)\n",
    "        v = F.unfold(v, kernel_size = self.kernel_size, padding = self.kernel_size // 2)\n",
    "\n",
    "        k, v = map(lambda t: rearrange(t, 'b (d j) n -> b n j d', d = self.dim_head), (k, v))\n",
    "\n",
    "        q = rearrange(q, 'b c ... -> b (...) c') * self.scale\n",
    "\n",
    "        sim = einsum('b i d, b i j d -> b i j', q, k)\n",
    "        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        full_out = einsum('b i j, b i j d -> b i d', attn, v)\n",
    "        full_out = rearrange(full_out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
    "\n",
    "        # add outputs of linear attention + conv like full attention\n",
    "\n",
    "        lin_out = self.nonlin(lin_out)\n",
    "        out = torch.cat((lin_out, full_out), dim = 1)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class FCANet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        chan_in,\n",
    "        chan_out,\n",
    "        reduction = 4,\n",
    "        width\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        freq_w, freq_h = ([0] * 8), list(range(8)) # in paper, it seems 16 frequencies was ideal\n",
    "        dct_weights = get_dct_weights(width, chan_in, [*freq_w, *freq_h], [*freq_h, *freq_w])\n",
    "        self.register_buffer('dct_weights', dct_weights)\n",
    "\n",
    "        chan_intermediate = max(3, chan_out // reduction)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(chan_in, chan_intermediate, 1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(chan_intermediate, chan_out, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = reduce(x * self.dct_weights, 'b c (h h1) (w w1) -> b c h1 w1', 'sum', h1 = 1, w1 = 1)\n",
    "        return self.net(x)\n",
    "\n",
    "class GlobalContext(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        chan_in,\n",
    "        chan_out\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.to_k = nn.Conv2d(chan_in, 1, 1)\n",
    "        chan_intermediate = max(3, chan_out // 2)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(chan_in, chan_intermediate, 1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(chan_intermediate, chan_out, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        context = self.to_k(x)\n",
    "        context = context.flatten(2).softmax(dim = -1)\n",
    "        out = einsum('b i n, b c n -> b c i', context, x.flatten(2))\n",
    "        out = out.unsqueeze(-1)\n",
    "        return self.net(out)\n",
    "\n",
    "\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, noise = None):\n",
    "        b = x.shape[0]\n",
    "        h = x.shape[2]\n",
    "        w = x.shape[3] \n",
    "        device = x.device\n",
    "\n",
    "        if not exists(noise):\n",
    "            noise = torch.randn(b, 1, h, w, device = device)\n",
    "\n",
    "        return x + self.weight * noise\n",
    "\n",
    "\n",
    "class PixelShuffleUpsample(nn.Module):\n",
    "    def __init__(self, dim, dim_out = None):\n",
    "        super().__init__()\n",
    "        dim_out = default(dim_out, dim)\n",
    "        conv = nn.Conv2d(dim, dim_out * 4, 1)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            conv,\n",
    "            nn.SiLU(),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "\n",
    "        self.init_conv_(conv)\n",
    "\n",
    "    def init_conv_(self, conv):\n",
    "        o, i, h, w = conv.weight.shape\n",
    "        conv_weight = torch.empty(o // 4, i, h, w)\n",
    "        nn.init.kaiming_uniform_(conv_weight)\n",
    "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
    "\n",
    "        conv.weight.data.copy_(conv_weight)\n",
    "        nn.init.zeros_(conv.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    \n",
    "class Blur(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        f = torch.Tensor([1, 2, 1])\n",
    "        self.register_buffer('f', f)\n",
    "    def forward(self, x):\n",
    "        f = self.f\n",
    "        f = f[None, None, :] * f [None, :, None]\n",
    "        return filter2d(x, f, normalized=True)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size = IMG_SIZE,\n",
    "        latent_dim = LATENT_DIM,\n",
    "        fmap_max = 512,\n",
    "        fmap_inverse_coef = 12,\n",
    "        transparent = False,\n",
    "        greyscale = False,\n",
    "        attn_res_layers = [32],\n",
    "        freq_chan_attn = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        resolution = log2(image_size)\n",
    "        assert is_power_of_two(image_size), 'image size must be a power of 2'\n",
    "\n",
    "        if transparent:\n",
    "            init_channel = 4\n",
    "        elif greyscale:\n",
    "            init_channel = 1\n",
    "        else:\n",
    "            init_channel = 3\n",
    "\n",
    "        fmap_max = default(fmap_max, latent_dim)\n",
    "\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, latent_dim * 2, 4),\n",
    "            nn.BatchNorm2d(latent_dim * 2),\n",
    "            nn.GLU(dim = 1)\n",
    "        )\n",
    "\n",
    "        num_layers = int(resolution) - 2\n",
    "        features = list(map(lambda n: (n,  2 ** (fmap_inverse_coef - n)), range(2, num_layers + 2)))\n",
    "        features = list(map(lambda n: (n[0], min(n[1], fmap_max)), features))\n",
    "        features = list(map(lambda n: 3 if n[0] >= 8 else n[1], features))\n",
    "        features = [latent_dim, *features]\n",
    "\n",
    "        in_out_features = list(zip(features[:-1], features[1:]))\n",
    "\n",
    "        self.res_layers = range(2, num_layers + 2)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.res_to_feature_map = dict(zip(self.res_layers, in_out_features))\n",
    "\n",
    "        self.sle_map = ((3, 7), (4, 8), (5, 9), (6, 10))\n",
    "        self.sle_map = list(filter(lambda t: t[0] <= resolution and t[1] <= resolution, self.sle_map))\n",
    "        self.sle_map = dict(self.sle_map)\n",
    "\n",
    "        self.num_layers_spatial_res = 1\n",
    "\n",
    "        for (res, (chan_in, chan_out)) in zip(self.res_layers, in_out_features):\n",
    "            image_width = 2 ** res\n",
    "            attn = None\n",
    "            if image_width in attn_res_layers:\n",
    "                attn = PreNorm(chan_in, LinearAttention(chan_in))\n",
    "\n",
    "            sle = None\n",
    "            if res in self.sle_map:\n",
    "                residual_layer = self.sle_map[res]\n",
    "                sle_chan_out = self.res_to_feature_map[residual_layer - 1][-1]\n",
    "\n",
    "                if freq_chan_attn:\n",
    "                    sle = FCANet(\n",
    "                        chan_in = chan_out,\n",
    "                        chan_out = sle_chan_out,\n",
    "                        width = 2 ** (res + 1)\n",
    "                    )\n",
    "                else:\n",
    "                    sle = GlobalContext(\n",
    "                        chan_in = chan_out,\n",
    "                        chan_out = sle_chan_out\n",
    "                    )\n",
    "\n",
    "            layer = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    PixelShuffleUpsample(chan_in),\n",
    "                    Blur(),\n",
    "                    Conv2dSame(chan_in, chan_out * 2, 4),\n",
    "                    Noise(),\n",
    "                    nn.BatchNorm2d(chan_out * 2),\n",
    "                    nn.GLU(dim = 1)\n",
    "                ),\n",
    "                sle,\n",
    "                attn\n",
    "            ])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(features[-1], init_channel, 3, padding = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b c -> b c () ()')\n",
    "        x = self.initial_conv(x)\n",
    "        x = F.normalize(x, dim = 1)\n",
    "\n",
    "        residuals = dict()\n",
    "\n",
    "        for (res, (up, sle, attn)) in zip(self.res_layers, self.layers):\n",
    "            if exists(attn):\n",
    "                x = attn(x) + x\n",
    "\n",
    "            x = up(x)\n",
    "\n",
    "            if exists(sle):\n",
    "                out_res = self.sle_map[res]\n",
    "                residual = sle(x)\n",
    "                residuals[out_res] = residual\n",
    "\n",
    "            next_res = res + 1\n",
    "            if next_res in residuals:\n",
    "                x = x * residuals[next_res]\n",
    "\n",
    "        return self.out_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# landmarks model\n",
    "\n",
    "def load_file_from_url(url, model_dir=None, progress=True, check_hash=False, file_name=None):\n",
    "    if model_dir is None:\n",
    "        hub_dir = get_dir()\n",
    "        model_dir = os.path.join(hub_dir, 'checkpoints')\n",
    "\n",
    "    try:\n",
    "        os.makedirs(model_dir)\n",
    "    except OSError as e:\n",
    "        if e.errno == errno.EEXIST:\n",
    "            # Directory already exists, ignore.\n",
    "            pass\n",
    "        else:\n",
    "            # Unexpected OSError, re-raise.\n",
    "            raise\n",
    "\n",
    "    parts = urlparse(url)\n",
    "    filename = os.path.basename(parts.path)\n",
    "    if file_name is not None:\n",
    "        filename = file_name\n",
    "    cached_file = os.path.join(model_dir, filename)\n",
    "    if not os.path.exists(cached_file):\n",
    "        sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
    "        hash_prefix = None\n",
    "        if check_hash:\n",
    "            r = HASH_REGEX.search(filename)  # r is Optional[Match[str]]\n",
    "            hash_prefix = r.group(1) if r else None\n",
    "        download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n",
    "\n",
    "    return cached_file\n",
    "\n",
    "\n",
    "def load_landmarks_model():\n",
    "    two_d_model_url = \"https://www.adrianbulat.com/downloads/python-fan/2DFAN4-cd938726ad.zip\"\n",
    "    return torch.jit.load(load_file_from_url(two_d_model_url)).eval().to(DEVICE)\n",
    "\n",
    "\n",
    "# Generator\n",
    "def load_generator() -> torch.nn.Module:\n",
    "    state_dict = torch.load(\"./model/generator_dict.pt\")\n",
    "    \n",
    "    new_generator = Generator()\n",
    "    new_generator.load_state_dict(state_dict)\n",
    "    return new_generator.to(DEVICE).eval()\n",
    "\n",
    "\n",
    "class DifferentiableLandmarks(torch.nn.Module):\n",
    "    def __init__(self, generator, landmarks_model, images: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.z = torch.nn.Parameter(pick_best_random_z(images, generator))\n",
    "        \n",
    "        self.landmarks_model = landmarks_model\n",
    "        self.generator = generator\n",
    "        self.generator.eval()\n",
    "        self.landmarks_model.eval()\n",
    "               \n",
    "        for p in self.generator.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        for p in self.landmarks_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.curr_image = self.generator(self.z)\n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "        self.curr_image = self.generator(self.z)\n",
    "        return self.landmarks_model(self.curr_image)\n",
    "    \n",
    "    def get_z(self) -> torch.Tensor:\n",
    "        return self.z.clone().detach().cpu()\n",
    "    \n",
    "    def print_are_training(self):\n",
    "        print(\"Are training:\")\n",
    "        print(f\"Generator: {self.generator.training}\")\n",
    "        print(f\"Landmarks: {self.landmarks_model.training}\")\n",
    "        print(f\"Model: {self.training}\")\n",
    "        \n",
    "        \n",
    "def pick_best_random_z(images: torch.Tensor, generator: torch.nn.Module, n=100, criterion=torch.nn.MSELoss()) -> torch.Tensor:\n",
    "    batch_size = images.shape[0]\n",
    "    z = torch.randn( (batch_size, LATENT_DIM), requires_grad=True, device=DEVICE)\n",
    "    z_best = z.clone()\n",
    "    loss_best = float(\"inf\")\n",
    "    \n",
    "    for _ in range(n):\n",
    "        z = torch.randn( (batch_size, LATENT_DIM), requires_grad=True, device=DEVICE)\n",
    "        images_gen = generator(z)\n",
    "        with torch.no_grad():\n",
    "            loss = criterion(images, images_gen)\n",
    "            if loss < loss_best:\n",
    "                z_best = z.clone()\n",
    "                loss_best = loss.clone()\n",
    "            \n",
    "    return z_best\n",
    "    \n",
    "        \n",
    "def find_z(images: torch.Tensor, lr=1e-2, iters=200) -> torch.Tensor:   \n",
    "    generator = load_generator().to(DEVICE)\n",
    "    landmarks_model = load_landmarks_model()\n",
    "    model = DifferentiableLandmarks(generator, landmarks_model, images)\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    y_target = landmarks_model(images).detach()\n",
    "    \n",
    "    for _ in range(iters):\n",
    "        y_pred = model()\n",
    "        loss = criterion(y_target, y_pred) + 5 * criterion(images, model.curr_image)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return  model.get_z()\n",
    "\n",
    "def prep_image(filename)-> torch.tensor:\n",
    "    image = Image.open(filename).convert(\"RGB\")\n",
    "    image_min_size = min(image.size)\n",
    "    transforms = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.CenterCrop(image_min_size),\n",
    "        v2.Resize((IMG_SIZE, IMG_SIZE), antialias=True),\n",
    "        v2.ToDtype(torch.float32, scale=True)\n",
    "    ])\n",
    "    return transforms(image).to(DEVICE)\n",
    "\n",
    "def create_outpath(pakcage_num: int, folder: str) -> str:\n",
    "    return f\"{folder}/package_{pakcage_num:04d}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 127/200 [7:20:18<4:13:05, 208.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(N_BATCHES)):\n\u001b[1;32m      6\u001b[0m     batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prep_image(p)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m path_names_permuted[(BATCH_SIZE\u001b[38;5;241m*\u001b[39mi \u001b[38;5;241m+\u001b[39m START_FROM):(BATCH_SIZE\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m START_FROM)]])\n\u001b[0;32m----> 7\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mfind_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m     z_list\u001b[38;5;241m.\u001b[39mappend(z)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m SAVE_EVERY_N_BATCHES \u001b[38;5;241m==\u001b[39m (SAVE_EVERY_N_BATCHES \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[0;32mIn[8], line 109\u001b[0m, in \u001b[0;36mfind_z\u001b[0;34m(images, lr, iters)\u001b[0m\n\u001b[1;32m    106\u001b[0m y_target \u001b[38;5;241m=\u001b[39m landmarks_model(images)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[0;32m--> 109\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_target, y_pred) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m criterion(images, model\u001b[38;5;241m.\u001b[39mcurr_image)\n\u001b[1;32m    111\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Documents/studia/magisterka/CKTinderator/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/studia/magisterka/CKTinderator/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 69\u001b[0m, in \u001b[0;36mDifferentiableLandmarks.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz)\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlandmarks_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurr_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/studia/magisterka/CKTinderator/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/studia/magisterka/CKTinderator/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "z_list = []\n",
    "saved_z = []\n",
    "package_num = 113  # change to get_last_package_num(OUTPUT_FOLDER) if you want to continue from last package\n",
    "\n",
    "for i in tqdm(range(N_BATCHES)):\n",
    "    batch = torch.cat([prep_image(p).unsqueeze(0) for p in path_names_permuted[(BATCH_SIZE*i + START_FROM):(BATCH_SIZE*(i+1) + START_FROM)]])\n",
    "    z = find_z(batch).numpy()\n",
    "    z_list.append(z)\n",
    "    \n",
    "    if i % SAVE_EVERY_N_BATCHES == (SAVE_EVERY_N_BATCHES - 1):\n",
    "        z_list = np.array(z_list).reshape((-1, 256))\n",
    "        np.save(create_outpath(package_num, OUTPUT_FOLDER), z_list)\n",
    "        saved_z.append(create_outpath(package_num, OUTPUT_FOLDER))\n",
    "        package_num += 1\n",
    "        z_list = []\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def prep_tensor_to_show(tensor):\n",
    "    return tensor.permute(1, 2, 0).detach().to('cpu').numpy()\n",
    "\n",
    "def show_tensor_picture(tensor, title=\"\"):\n",
    "    im_arr = prep_tensor_to_show(tensor)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(im_arr)\n",
    "    \n",
    "\n",
    "def plot_images(images, labels, nrows, ncols, step=1, title=\"\", figsize=[8, 4]):\n",
    "    \"\"\"Plot nrows x ncols images from images and set labels as titles.\"\"\"\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        idx = i * step\n",
    "        if images[i].shape[-1] == 1:\n",
    "            ax.imshow((images[i]).reshape(images[idx].shape[0], images[idx].shape[1]))\n",
    "        else:\n",
    "            ax.imshow((images[idx]))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(labels[idx])\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"{title}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx, last_idx = START_FROM, (BATCH_SIZE * N_BATCHES) + START_FROM\n",
    "files_that_were_infered = [np.array(Image.open(p)) for p in path_names_permuted[start_idx:last_idx]]\n",
    "\n",
    "generator = load_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = np.concatenate([np.load(z) for z in saved_z])\n",
    "generated_images = [generator(torch.tensor(z).unsqueeze(0).to(DEVICE)).detach().to('cpu')[0].permute((1,2,0)) for z in zs]\n",
    "\n",
    "for generated_image, original_image in zip(generated_images, files_that_were_infered):\n",
    "    plot_images([original_image, generated_image], [\"Original\", \"Generated\"], 1, 2, figsize=[8, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the generator\n",
    "\n",
    "curr_generator = torch.load(\"./model/generator.pt\").eval().cuda(0)\n",
    "curr_gen_state_dict = curr_generator.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_generator = Generator()\n",
    "new_generator.load_state_dict(curr_gen_state_dict)\n",
    "new_generator.cuda(0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_latent_vectors = torch.randn( (1, LATENT_DIM) ).cuda(0)\n",
    "generated_images = new_generator(random_latent_vectors)\n",
    "show_tensor_picture(generated_images[0], title=\"Random Freddie Mercury\")\n",
    "print(generated_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_images = curr_generator(random_latent_vectors)\n",
    "show_tensor_picture(generated_images[0], title=\"Random Freddie Mercury\")\n",
    "print(generated_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.load(\"./model/generator_dict.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(curr_gen_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(curr_gen_state_dict, \"./model/generator_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"./model/generator_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
